<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How AI Enabled Our QA Team to Build CI/CD Pipelines Without Waiting for DevOps</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,700&family=DM+Serif+Display&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <style>
    :root {
      --ink: #1a1a2e;
      --ink-soft: #3d3d56;
      --ink-muted: #6b6b82;
      --surface: #fefdfb;
      --surface-warm: #f7f5f0;
      --accent: #c45d3e;
      --accent-glow: #fff0eb;
      --rule: #e2dfd8;
      --serif: 'DM Serif Display', Georgia, serif;
      --sans: 'DM Sans', -apple-system, sans-serif;
      --mono: 'JetBrains Mono', monospace;
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }

    body {
      font-family: var(--sans);
      background: var(--surface);
      color: var(--ink);
      -webkit-font-smoothing: antialiased;
    }

    .article-header {
      max-width: 720px;
      margin: 0 auto;
      padding: 80px 32px 48px;
    }

    .meta-row {
      display: flex;
      align-items: center;
      gap: 16px;
      margin-bottom: 32px;
      font-size: 13px;
      font-weight: 500;
      color: var(--ink-muted);
      letter-spacing: 0.3px;
      text-transform: uppercase;
    }

    .meta-row .tag {
      background: var(--accent-glow);
      color: var(--accent);
      padding: 4px 12px;
      border-radius: 4px;
      font-weight: 600;
      font-size: 11px;
      letter-spacing: 0.8px;
    }

    .article-header h1 {
      font-family: var(--serif);
      font-size: clamp(2rem, 5vw, 2.6rem);
      line-height: 1.2;
      color: var(--ink);
      font-weight: 400;
    }

    .article-body {
      max-width: 720px;
      margin: 0 auto;
      padding: 0 32px 80px;
    }

    .article-body p {
      font-size: 17px;
      line-height: 1.82;
      color: var(--ink-soft);
      margin-bottom: 24px;
    }

    .article-body h2 {
      font-family: var(--serif);
      font-size: 1.55rem;
      color: var(--ink);
      margin: 48px 0 20px;
      font-weight: 400;
      line-height: 1.3;
    }

    .article-body strong {
      color: var(--ink);
      font-weight: 600;
    }

    .article-body code {
      font-family: var(--mono);
      font-size: 14px;
      background: var(--surface-warm);
      padding: 2px 6px;
      border-radius: 3px;
    }

    .article-body em {
      font-style: italic;
    }

    .article-body hr {
      border: none;
      height: 1px;
      background: var(--rule);
      margin: 48px 0;
    }

    .article-body blockquote {
      margin: 40px 0;
      padding: 0 0 0 28px;
      border-left: 3px solid var(--accent);
    }

    .article-body blockquote p {
      font-family: var(--serif);
      font-size: 1.2rem;
      line-height: 1.55;
      color: var(--ink);
      margin-bottom: 8px;
    }

    .article-body blockquote .attr {
      font-family: var(--sans);
      font-size: 13px;
      color: var(--ink-muted);
      font-weight: 500;
      letter-spacing: 0.3px;
    }

    .about-section {
      max-width: 720px;
      margin: 0 auto;
      padding: 0 32px 80px;
    }

    .about-section h2 {
      font-family: var(--serif);
      font-size: 1.3rem;
      color: var(--ink);
      margin-bottom: 12px;
      font-weight: 400;
    }

    .about-section p {
      font-size: 15px;
      line-height: 1.7;
      color: var(--ink-muted);
      margin-bottom: 16px;
    }

    .about-section p em {
      font-style: italic;
    }

    .about-section a {
      color: var(--accent);
      text-decoration: underline;
      text-underline-offset: 3px;
    }

    @media (max-width: 640px) {
      .article-header { padding: 48px 20px 32px; }
      .article-body { padding: 0 20px 60px; }
      .about-section { padding: 0 20px 60px; }
    }
  </style>
</head>
<body>

  <header class="article-header">
    <div class="meta-row">
      <span class="tag">Engineering</span>
      <span>February 2026 · 9 min read</span>
    </div>
    <h1>How AI Enabled Our QA Team to Build CI/CD Pipelines Without Waiting for DevOps</h1>
  </header>

  <article class="article-body">

    <hr>

    <p>There's a moment in the life of every fintech QA automation effort when the tests are ready — and the infrastructure to run them isn't.</p>

    <p>The tests exist. They verify transaction flows end-to-end. They catch breaking changes across microservices. They run on the automation engineer's laptop, deliver clear results, and nobody else ever sees them.</p>

    <p>Not because the tests are bad. Not because the team doesn't value automation. But because DevOps is doing exactly what they should be doing — securing production infrastructure, managing environments, keeping the platform stable for real users with real money — and the QA pipeline is correctly further down the priority list.</p>

    <p>This is the story of how we broke that pattern on a MiCAR-regulated crypto-fiat platform by using AI to build production-grade CI/CD pipelines from the QA side. Not a workaround. Not a hack. A proper GitHub Actions setup — with daily scheduled runs, structured Slack reporting, and conditional execution logic — reviewed and approved by DevOps, built without ever blocking them.</p>

    <p>What we learned in the process applies to any fintech team where automation value is locked behind infrastructure that hasn't been built yet.</p>

    <hr>

    <h2>The Platform That Tested Our Testing</h2>

    <p>The client's product sits at the intersection of traditional finance and digital assets. Two layers: a DASP (Digital Asset Service Provider) platform where end-users manage crypto wallets, initiate transactions, and track balances — and an EMI (E-Money Institution) administration layer where operators configure banking integrations, manage compliance workflows, and control operational parameters.</p>

    <p>Dozens of microservices. Multiple third-party integrations with payment processors and banking partners, each behaving differently in test environments versus production. The kind of architecture where a backend developer adds a required field to one onboarding endpoint and three downstream services quietly break.</p>

    <p>We engineered what we call a <strong>Platinum Journey</strong> test suite for this platform — a focused set of 5–10 end-to-end tests covering the core user journeys: account creation, account linking, send, receive, swap, and top-up. The design principle was deliberate restraint: not exhaustive coverage, but a daily stability signal. When Platinum Journey passes, the system's core transaction flows are intact. When it fails, something changed — and we know before users discover it.</p>

    <p>The suite worked. It caught real issues. On one occasion, a backend developer added new mandatory fields to the onboarding endpoint, and Platinum Journey immediately failed with a 400 error. We flagged that the frontend hadn't implemented the corresponding changes. Caught before deployment, not after. That's exactly what the tests were designed to do.</p>

    <p>The problem was that these tests ran when someone remembered to run them. Which, under sprint pressure, was increasingly "not often enough."</p>

    <hr>

    <h2>The Pipeline Gap</h2>

    <p>One DevOps engineer on the project. His priority was production infrastructure and security — exactly where it should be for a platform handling digital asset transactions under MiCAR regulatory requirements. Environment configuration. Monitoring. Deployment pipelines for the development team. All of it more urgent than configuring a scheduled test runner for QA.</p>

    <p>This isn't a criticism. It's reality.</p>

    <p>We see this pattern across fintech scale-ups. DevOps capacity is finite. Infrastructure and security take precedence — correctly. QA automation pipelines wait in the backlog. The tests exist, the delivery mechanism doesn't, and automation value stays locked on one engineer's machine.</p>

    <p>The typical response is to wait. File a ticket. Remind DevOps during standup. Wait longer. Eventually, the pipeline gets built — weeks or months later — and the team retroactively starts getting value from automation they wrote long ago.</p>

    <p>We decided not to wait.</p>

    <hr>

    <h2>Adjacent Expertise and the AI Bridge</h2>

    <p>Our lead automation QA engineer had built CI/CD pipelines for test frameworks on three previous projects. All on Jenkins. He knew the architecture intimately: scheduled triggers, conditional execution paths, structured Slack notifications with pass/fail breakdowns and change attribution, failure handling that routes differently based on test type.</p>

    <p>He knew exactly what "done" looked like. He could describe the pipeline's behavior in detail — what triggers when, what notifies whom, what happens on failure versus success.</p>

    <p>This project ran GitHub Actions. Different syntax. Different configuration model. Different ecosystem of plugins and marketplace actions. The mental model was identical; the implementation language was foreign.</p>

    <p>This turns out to be a very specific class of engineering problem — and one where AI is genuinely effective. You understand the domain deeply. You know the target architecture. You can evaluate whether an output is correct. What you lack is fluency in one particular technology's idioms.</p>

    <p>AI excels at exactly this kind of translation. You describe architectural intent; it generates the platform-specific implementation. You review against your mental model; it iterates on your feedback. The knowledge stays with you. The syntax comes from AI.</p>

    <p>Our engineer used Augment Code — an AI coding assistant — and built the pipeline over several focused sessions. Not by saying "build me a CI/CD pipeline" and accepting whatever came back. By describing specific behaviors, reviewing every output, and making judgment calls at each step about what fit the project's actual requirements.</p>

    <hr>

    <h2>What We Built (Specifically)</h2>

    <p>Three deliverables came out of this work.</p>

    <p><strong>Daily automated test execution.</strong> Platinum Journey runs on schedule every morning. No manual triggers. No dependency on anyone remembering to run it. The team wakes up to results, the same way they wake up to Slack messages and email — it's just part of the daily workflow.</p>

    <p><strong>Structured Slack reporting.</strong> Not a basic webhook that says "tests passed" or "tests failed." The notifications include which specific tests passed, which failed, who pushed the last code changes before the run, and direct links to detailed reports. The engineering team sees automation results where they already work — not buried in a CI/CD dashboard that nobody checks unless something is visibly broken.</p>

    <p>This matters more than it sounds. Before the pipeline, automation results were a conversation: "Hey, did you run the tests? What happened?" After the pipeline, they were a signal: visible, passive, shared. Automation stopped being "the QA team's thing" and became a shared engineering heartbeat.</p>

    <p><strong>Conditional pipeline logic.</strong> Different test suites execute under different conditions. Platinum Journey runs daily for stability monitoring — it's the canary. Service-level suites trigger on specific deployment events when you need deeper coverage. Failure handling routes differently based on test type and severity — a Platinum Journey failure is a different kind of alarm than a service suite edge case failure.</p>

    <p>None of this was AI operating autonomously. The engineer provided the architecture. AI provided the implementation velocity. The engineer validated every output.</p>

    <hr>

    <h2>The Honest Part: Where This Works and Where It Doesn't</h2>

    <p>We discussed this candidly within the team after the pipeline was running. The question wasn't "did AI help?" — it obviously did. The question was "why did it work here, and would it work for everyone?"</p>

    <p>The answer comes down to one concept: <strong>adjacent expertise</strong>.</p>

    <p>Our engineer understood pipeline architecture. He'd built the same logical structure three times before in Jenkins. He knew the scheduling model, the notification patterns, the conditional routing, the failure handling. What he lacked was GitHub Actions syntax — not CI/CD knowledge.</p>

    <p>AI is effective at this kind of translation because the person prompting it can evaluate the output. When the generated YAML had a configuration error, our engineer recognized it — not because he knew GitHub Actions, but because he knew what correct pipeline behavior looked like and could tell the output wouldn't produce it.</p>

    <p>Now consider the alternative. If someone with no CI/CD experience — say, a manual QA engineer who's never configured a pipeline on any platform — tried the same approach, AI would still generate something. It might even run. But they wouldn't know whether the notification logic handled edge cases. They wouldn't know whether the conditional execution was robust. They wouldn't know what "good" looks like, which means they can't tell when AI gives them something mediocre.</p>

    <blockquote>
      <p>"AI still delivers something basic when you don't have deep experience — you won't be blocked entirely. But building something production-grade requires knowing what good looks like."</p>
      <span class="attr">— QA Automation Lead, KindGeek</span>
    </blockquote>

    <p>This is the nuance that gets lost in most AI-in-engineering conversations. AI amplifies existing expertise. It doesn't generate expertise from nothing. A senior engineer with AI is significantly faster. A junior engineer with AI is still junior — just with better autocomplete.</p>

    <p>That distinction matters, and we'd rather be honest about it than pretend AI is a universal accelerator.</p>

    <hr>

    <h2>What Changed on the Project</h2>

    <p>The practical impact was significant — not because the pipeline was technically remarkable, but because it removed a constraint that had been quietly limiting the team's entire automation investment.</p>

    <p><strong>DevOps stayed focused on what mattered.</strong> Our QA engineer built the pipeline independently. When it was ready, DevOps reviewed it for security and alignment with the project's CI/CD standards — a two-hour review rather than a multi-week build. Neither team blocked the other. DevOps capacity stayed on infrastructure. QA automation shipped on its own timeline.</p>

    <p><strong>Regression validation dropped from hours to minutes.</strong> When the team needed to reconfigure the soft ledger — swapping accounts and backend settings — they ran Platinum Journey after the change and confirmed all core transaction flows still worked. Fifteen to twenty minutes. No manual retesting of every flow. No weekend spent worrying about whether the change broke something downstream. That kind of rapid validation only exists when the pipeline exists.</p>

    <p><strong>The pattern became repeatable.</strong> Once the Platinum Journey pipeline was operational, extending it for additional service-level test suites was configuration, not construction. The infrastructure was in place. Adding a new suite meant adding a workflow file and configuring its triggers — not filing a DevOps ticket and waiting three sprints.</p>

    <hr>

    <h2>The Bigger Picture: Where AI Belongs in Test Automation</h2>

    <p>This experience confirmed something we've observed across multiple fintech projects: <strong>the highest-value use of AI in test automation isn't writing tests. It's eliminating friction in everything around the tests.</strong></p>

    <p>Writing a meaningful test for a MiCAR-compliant custody flow, or a multi-party cross-border transaction, or a reconciliation process that touches three third-party payment processors — that requires deep understanding of the business domain, the regulatory context, the data flows, the integration quirks. That's where human expertise is non-negotiable. AI can't understand why the system behaves differently for a German customer versus a UK customer unless someone tells it — and even then, it might decide to "improve" the logic.</p>

    <p>But building the pipeline that runs those tests? Refactoring endpoint definitions across 30 service classes into a centralized registry so you can measure automation coverage? Generating data providers from a Swagger specification? Replacing hardcoded magic strings with proper Java enums across a 900-endpoint project? These are tasks where the engineer knows the target state but the manual execution is tedious, repetitive, and error-prone.</p>

    <p>On this same project, we used AI for <strong>framework refactoring</strong> — migrating all endpoint path definitions into a centralized registry to build automated coverage metrics. The registry let us compare automated endpoints against the total Swagger spec and quantify exactly where coverage gaps existed. Doing that migration manually — copying paths from dozens of individual controller classes, reformatting them, adding metadata — would have been hours of mind-numbing copy-paste with inevitable human errors. AI did it consistently and quickly.</p>

    <p>We also used AI for <strong>code quality enforcement</strong> — post-writing review passes that catch the things you'd fix if you had time. Hardcoded <code>"primary"</code> strings that should be Java enums. Missing data providers for field validation tests. Inconsistent method naming that accumulated across sprints because nobody had bandwidth for a cleanup pass.</p>

    <p>Same pattern every time: engineer provides architectural intent and domain context. AI provides implementation velocity. Engineer validates the output.</p>

    <hr>

    <h2>What We'd Tell Every Fintech QA Team</h2>

    <p>If your automation is sitting idle because the pipeline isn't configured — and you have CI/CD experience from any platform — build it yourself with AI bridging the technology gap. Don't wait for DevOps. Don't wait for "next sprint." The tests you've already written are an asset generating zero returns until they run automatically.</p>

    <p>But be honest about what AI can and can't do in this context.</p>

    <p><strong>AI is effective when you know the architecture and need the syntax.</strong> Translation between platforms — Jenkins to GitHub Actions, CircleCI to GitLab CI, or any similar migration — is the ideal use case. You know what correct looks like. AI generates the implementation. You validate.</p>

    <p><strong>AI is less effective when you don't know the domain.</strong> If you've never built a pipeline, AI can scaffold something basic, but you won't be able to evaluate whether it's production-grade. Start by understanding CI/CD fundamentals. Then let AI accelerate the implementation.</p>

    <p><strong>AI improvises, and you need to catch it.</strong> On our project, AI renamed an endpoint from <code>getAddress</code> to <code>getAddresses</code> because it decided the plural was more appropriate. It invented request fields that didn't exist in the API. It made assumptions about business logic that were wrong. Every AI-generated output requires genuine code review — not approval, review.</p>

    <p><strong>Start with infrastructure, not tests.</strong> AI's clearest value in automation engineering is in CI/CD pipelines, reporting, boilerplate generation, and refactoring. Well-defined tasks with easily verifiable output. Save the test-writing for cases where you have the framework context set up to make AI genuinely useful.</p>

    <p><strong>The goal is to raise the floor, not replace the engineer.</strong> AI under sprint pressure means your code quality doesn't degrade as fast. Enum generation, data provider creation, naming consistency, method extraction — the things you'd do if you had time. AI makes "if you had time" happen.</p>

  </article>

  <hr style="border:none;height:1px;background:#e2dfd8;max-width:720px;margin:0 auto;padding:0 32px;">

  <div class="about-section">
    <h2>About KindGeek</h2>
    <p>KindGeek is a fintech-specialized engineering team with 10+ years of experience building payment platforms, neobanks, and digital asset infrastructure. With 200+ engineers, ISO 9001 and ISO 27001 certifications, and 100+ fintech projects delivered, we accelerate time-to-market for fintech scale-ups and enterprises across the UK, DACH region, and Nordics.</p>
    <p><em>Running test automation that nobody sees the results of? <a href="https://kindgeek.com/contact">Let's talk</a> — we've been there, and we've built the way out.</em></p>
  </div>

</body>
</html>
